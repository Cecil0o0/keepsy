//! This is a classic lexer forms the first phase of a compiler frontend in processing, a rule-based program which performs lexical tokenization, a conversion of a given text into (syntactically or semantically) meaningful lexical tokens belonging to categories defined by lexer.
//! It is NOT aware of syntax-level information and is only responsible for lexical tokenization.
//! If you want to know more about lexical analysis, please refer to the following link:
//! https://en.wikipedia.org/wiki/Lexical_analysis
